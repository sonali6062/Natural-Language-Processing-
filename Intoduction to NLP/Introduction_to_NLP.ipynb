{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJmAuItGPkTv",
        "outputId": "126d82ad-5d7f-4608-cd34-7c50c8185ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk # Install the Natural Language Toolkit (NLTK) library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # Import the NLTK library"
      ],
      "metadata": {
        "id": "le2U-yv3PvGy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK is very large in size. So we've to sometimes manually download it using punkt_tab."
      ],
      "metadata": {
        "id": "zViWta8yTJOL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a78959fb",
        "outputId": "8fb79eb3-1233-4780-99c8-ff6e639beff0"
      },
      "source": [
        "nltk.download('punkt_tab')\n",
        "# nltk.download() function from the NLTK library to download a specific dataset or model called 'punkt_tab'.\n",
        "# The punkt_tab resource is a tokenizer model used for splitting text into sentences and words, and it's required by functions like sent_tokenize and word_tokenize.\n",
        "# Downloading it makes these functions available for use."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt=\"Hello Everyone. Campus is hoping you guys are doing well.\" # Define a sample text string\n",
        "txt # Display the text string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5V2By8qCP6XH",
        "outputId": "847f20e7-3d6f-46ec-d275-5c8ebb38193d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Everyone. Campus is hoping you guys are doing well.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt.split('.') # Split the text into a list of strings based on the period '.' delimiter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW_eRsO6QQnx",
        "outputId": "4c0fd2bb-c5d1-4f9b-fac6-2358e1184d02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Everyone', ' Campus is hoping you guys are doing well', '']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt.split(' ') # Split the text into a list of strings based on the space ' ' delimiter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr-D4GRFQThc",
        "outputId": "2c0ad0cd-8be2-40f1-9cb7-e594fdf6c97b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Everyone.',\n",
              " 'Campus',\n",
              " 'is',\n",
              " 'hoping',\n",
              " 'you',\n",
              " 'guys',\n",
              " 'are',\n",
              " 'doing',\n",
              " 'well.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(txt.split('.')) # Calculate the number of elements in the list obtained by splitting the text by '.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt8S60cpQbBX",
        "outputId": "1bafb34a-e06f-4e9f-fcb6-e82020a88077"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(txt.split(' ')) # Calculate the number of elements in the list obtained by splitting the text by space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKcs5YvdQp0S",
        "outputId": "d34df9a1-418f-4fac-eb62-899eaee5fa07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize # Import word_tokenize and sent_tokenize functions from nltk.tokenize\n",
        "# These functions are used for tokenizing text into words and sentences respectively."
      ],
      "metadata": {
        "id": "USjkUWAbQsRv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(txt) # Tokenize the text into words using NLTK's word_tokenize function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy829UiSQ85P",
        "outputId": "886ac32c-9177-42f1-dfe5-3ea3b59b81de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Everyone',\n",
              " '.',\n",
              " 'Campus',\n",
              " 'is',\n",
              " 'hoping',\n",
              " 'you',\n",
              " 'guys',\n",
              " 'are',\n",
              " 'doing',\n",
              " 'well',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(txt): # Iterate through each word obtained from tokenizing the text\n",
        "  print(word) # Print each word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B8HGiCmRsIt",
        "outputId": "08a4ca55-5166-43a3-a4ee-d8526326a76a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Everyone\n",
            ".\n",
            "Campus\n",
            "is\n",
            "hoping\n",
            "you\n",
            "guys\n",
            "are\n",
            "doing\n",
            "well\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(txt): # Iterate through each word obtained from tokenizing the text\n",
        "  if(word!='.'): # Check if the word is not a period '.'\n",
        "    print(word) # Print the word if it's not a period"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfjfP2unSDD8",
        "outputId": "73e894c9-96ea-4b11-8644-3abc8d046894"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Everyone\n",
            "Campus\n",
            "is\n",
            "hoping\n",
            "you\n",
            "guys\n",
            "are\n",
            "doing\n",
            "well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(txt) # Tokenize the text into sentences using NLTK's sent_tokenize function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRPKJTIyRAVK",
        "outputId": "f0934b15-69b5-429b-9d58-2665facce2f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Everyone.', 'Campus is hoping you guys are doing well.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDc-fe-sRpHR"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}
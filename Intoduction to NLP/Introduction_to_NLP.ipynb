{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJmAuItGPkTv",
        "outputId": "5df8d4ca-2b6a-447d-95f9-66d00d2ae2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk # Install the Natural Language Toolkit (NLTK) library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # Import the NLTK library"
      ],
      "metadata": {
        "id": "le2U-yv3PvGy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK is very large in size. So we've to sometimes manually download it using punkt_tab."
      ],
      "metadata": {
        "id": "zViWta8yTJOL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a78959fb",
        "outputId": "3c154922-3ffb-4378-bd24-721654f51d76"
      },
      "source": [
        "nltk.download('punkt_tab')\n",
        "# nltk.download() function from the NLTK library to download a specific dataset or model called 'punkt_tab'.\n",
        "# The punkt_tab resource is a tokenizer model used for splitting text into sentences and words, and it's required by functions like sent_tokenize and word_tokenize.\n",
        "# Downloading it makes these functions available for use."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt=\"Hello Everyone. Campus is hoping you guys are doing well.\" # Define a sample text string\n",
        "txt # Display the text string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5V2By8qCP6XH",
        "outputId": "b71f5a19-cbae-49af-d11d-b95195a403e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Everyone. Campus is hoping you guys are doing well.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt.split('.') # Split the text into a list of strings based on the period '.' delimiter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW_eRsO6QQnx",
        "outputId": "cd8f44eb-b51a-46b5-8ed1-b3ae532e6b0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Everyone', ' Campus is hoping you guys are doing well', '']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt.split(' ') # Split the text into a list of strings based on the space ' ' delimiter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr-D4GRFQThc",
        "outputId": "b302ddc7-805f-4ee4-8692-e22556c47cde"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Everyone.',\n",
              " 'Campus',\n",
              " 'is',\n",
              " 'hoping',\n",
              " 'you',\n",
              " 'guys',\n",
              " 'are',\n",
              " 'doing',\n",
              " 'well.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(txt.split('.')) # Calculate the number of elements in the list obtained by splitting the text by '.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt8S60cpQbBX",
        "outputId": "7d9b26da-6781-421b-c21c-76a620b6a8e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(txt.split(' ')) # Calculate the number of elements in the list obtained by splitting the text by space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKcs5YvdQp0S",
        "outputId": "d81d2acf-a5fe-4ded-95d2-010f742c10e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize # Import word_tokenize and sent_tokenize functions from nltk.tokenize\n",
        "# These functions are used for tokenizing text into words and sentences respectively."
      ],
      "metadata": {
        "id": "USjkUWAbQsRv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(txt) # Tokenize the text into words using NLTK's word_tokenize function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy829UiSQ85P",
        "outputId": "8e4753ae-cd7d-449a-d117-fc1535c0d00e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Everyone',\n",
              " '.',\n",
              " 'Campus',\n",
              " 'is',\n",
              " 'hoping',\n",
              " 'you',\n",
              " 'guys',\n",
              " 'are',\n",
              " 'doing',\n",
              " 'well',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(txt): # Iterate through each word obtained from tokenizing the text\n",
        "  print(word) # Print each word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B8HGiCmRsIt",
        "outputId": "a156844f-6b88-470e-8926-3453cf8b6479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Everyone\n",
            ".\n",
            "Campus\n",
            "is\n",
            "hoping\n",
            "you\n",
            "guys\n",
            "are\n",
            "doing\n",
            "well\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(txt): # Iterate through each word obtained from tokenizing the text\n",
        "  if(word!='.'): # Check if the word is not a period '.'\n",
        "    print(word) # Print the word if it's not a period"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfjfP2unSDD8",
        "outputId": "26646760-a16f-4ce0-9a01-d5610882e9e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Everyone\n",
            "Campus\n",
            "is\n",
            "hoping\n",
            "you\n",
            "guys\n",
            "are\n",
            "doing\n",
            "well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(txt) # Tokenize the text into sentences using NLTK's sent_tokenize function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRPKJTIyRAVK",
        "outputId": "1dbd79d8-c3d4-48fb-b940-4d01a92d49c2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Everyone.', 'Campus is hoping you guys are doing well.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lemmitisation\n",
        "Techniques in NLP to reduce the vocab size(how many unique words we're having in the whole string)"
      ],
      "metadata": {
        "id": "xSt8RZw5UIK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbBcLmoaUHyY",
        "outputId": "74c0b553-521d-4923-b519-1e191e404399"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules for text normalization\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer # Import WordNetLemmatizer and PorterStemmer for text normalization"
      ],
      "metadata": {
        "id": "fDc-fe-sRpHR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stemmer and lemmatizer\n",
        "stem=PorterStemmer()\n",
        "lam= WordNetLemmatizer()\n",
        "#both are used to find the root words but in lemmatisation root word make sense but in stemming it might may not make sense(not available in dictionary)"
      ],
      "metadata": {
        "id": "nSYr9quPU2Na"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmitization"
      ],
      "metadata": {
        "id": "0CJSjIR3ZCPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-** Lemmatization, on the other hand, is a more sophisticated process that reduces words to their base or dictionary form, which is called a \"lemma\". The lemma is always a valid word.\n",
        "\n",
        "**2-** Lemmatization considers the context and part of speech of a word to determine its lemma. For example, the words \"running\", \"runs\", and \"ran\" would be lemmatized to \"run\", while \"better\" would be lemmatized to \"good\".\n",
        "\n"
      ],
      "metadata": {
        "id": "p-0w5oo6aqcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate lemmatization\n",
        "print(lam.lemmatize('change')) # Lemmatize the word 'change'\n",
        "print(lam.lemmatize('changes')) # Lemmatize the word 'changes'\n",
        "print(lam.lemmatize('changer')) # Lemmatize the word 'changer'\n",
        "print(lam.lemmatize('changed')) # Lemmatize the word 'changed'\n",
        "print(lam.lemmatize('changess')) # Lemmatize the word 'changess'\n",
        "#less data loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29mAVz24Y9A7",
        "outputId": "3c44530a-e058-4238-fdc9-efdd1eedd751"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change\n",
            "change\n",
            "changer\n",
            "changed\n",
            "changess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming"
      ],
      "metadata": {
        "id": "RNLPsK9iZlst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-** Stemming is a process that reduces\n",
        "words to their root or base form, which is called a \"stem\". The stem might not be an actual word in the dictionary.\n",
        "\n",
        "**2-** Stemming is a cruder process and often results in words that are not linguistically correct but are sufficient for many text processing tasks. For example, the words \"running\", \"runs\", and \"ran\" might all be reduced to the stem \"run\"."
      ],
      "metadata": {
        "id": "fVJPTLSWbBVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate stemming\n",
        "print(stem.stem('change')) # Stem the word 'change'\n",
        "print(stem.stem('changes')) # Stem the word 'changes'\n",
        "print(stem.stem('changer')) # Stem the word 'changer'\n",
        "print(stem.stem('changed')) # Stem the word 'changed'\n",
        "print(stem.stem('changing')) # Stem the word 'changing'\n",
        "#Huge data loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReFII_QfZaEa",
        "outputId": "4e672361-c028-44d7-fe20-e3aebfa8c040"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chang\n",
            "chang\n",
            "changer\n",
            "chang\n",
            "chang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison\n",
        "In essence, stemming is faster and simpler, while lemmatization is more accurate and produces linguistically correct results, but it is also more computationally expensive."
      ],
      "metadata": {
        "id": "S_4fIovhb0GO"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMKraKy6g-g3",
        "outputId": "2d103850-062f-40b6-f3dd-9e4062f5f79b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the tensorflow library\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the tensorflow library and the Tokenizer class\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "OzBUPeDVhGeI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to Sequences"
      ],
      "metadata": {
        "id": "cLnn9sDckwDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Tokenizer with a vocabulary size of 10\n",
        "tok = Tokenizer(num_words=10)"
      ],
      "metadata": {
        "id": "PDD0Ld6dhkll"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a corpus of text data\n",
        "corp=['coffee is hot','water is cold']"
      ],
      "metadata": {
        "id": "6GrkEKPxjYqB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the corpus\n",
        "corp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NYGS3PyjxvD",
        "outputId": "1022cd68-98c7-4e3a-989c-62460f631a03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['coffee is hot', 'water is cold']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the tokenizer on the corpus to build the vocabulary\n",
        "tok.fit_on_texts(corp)"
      ],
      "metadata": {
        "id": "w-x0-Kh9kg5y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the word index, which maps words to their integer IDs\n",
        "tok.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH2nMVurjhT9",
        "outputId": "e362d9c9-f68f-4376-930d-887d69d2a9e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the corpus texts to sequences of integer IDs\n",
        "tok.texts_to_sequences(corp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g1aD7gAkCF7",
        "outputId": "e9895394-c416-4bbe-bd0e-3259651c0362"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1, 3], [4, 1, 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert new texts to sequences using the fitted tokenizer\n",
        "tok.texts_to_sequences(['water is hot','black coffee is cold'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yfIf6vLkKjt",
        "outputId": "5518ddd7-12c9-4725-e310-795123d81faf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 1, 3], [2, 1, 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding OVV"
      ],
      "metadata": {
        "id": "ojvl9GoylACI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OOV stands for Out-Of-Vocabulary.**\n",
        "\n",
        "In Natural Language Processing (NLP), it refers to words that don’t exist in the vocabulary your model or tokenizer was trained on.\n",
        "\n",
        "For example:\n",
        "\n",
        "Suppose your model’s vocabulary = {this, is, a, good, time, talk}\n",
        "\n",
        "Your input sentence = \"This is not a good time to talk\"\n",
        "\n",
        "The word \"not\" is not in the vocabulary → so it becomes an OOV token (like <OOV> or UNK).\n",
        "\n",
        "This is important because models can’t handle unknown words directly. Instead, they replace them with a placeholder token (<OOV>) so the model knows \"something is here, but I don’t know what it is.\""
      ],
      "metadata": {
        "id": "g1ziY97cnuGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Tokenizer with an out-of-vocabulary (OOV) token\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok=Tokenizer(oov_token='black')\n",
        "corp=['coffee is hot','water is cold']"
      ],
      "metadata": {
        "id": "olwf15SBkRYH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the corpus\n",
        "corp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFm1o3U6lXd0",
        "outputId": "6af3a825-cef7-454d-9409-0e7185decd11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['coffee is hot', 'water is cold']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the tokenizer on the corpus and print the word index\n",
        "tok.fit_on_texts(corp)\n",
        "print(tok.word_index)\n",
        "# Convert new texts to sequences, demonstrating OOV token handling\n",
        "tok.texts_to_sequences(['water is hot','black coffee is cold'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VicSa1-JlaCc",
        "outputId": "6d81f19d-2a22-4984-e77e-c040a00705e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'black': 1, 'is': 2, 'coffee': 3, 'hot': 4, 'water': 5, 'cold': 6}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 2, 4], [1, 3, 2, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limiting the number of words"
      ],
      "metadata": {
        "id": "DDlVHYMyl60P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Tokenizer and limit the vocabulary size\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok=Tokenizer(num_words=5)\n",
        "corp=['coffee is hot','water is cold']\n",
        "tok.fit_on_texts(corp)\n",
        "print(tok.word_index)\n",
        "# Convert new texts to sequences, observing the effect of limited vocabulary\n",
        "tok.texts_to_sequences(['water is hot','black coffee is cold'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCK58Gc4ltqN",
        "outputId": "b4b8d893-39be-467b-eb33-c2925208d000"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 1, 3], [2, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cold is not taken as the index for it is 5 that is leading to exceeding of word_length"
      ],
      "metadata": {
        "id": "9-gG3EtMmXvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Tokenizer and further limit the vocabulary size\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok=Tokenizer(num_words=3)\n",
        "corp=['coffee is hot','water is cold']\n",
        "tok.fit_on_texts(corp)\n",
        "print(tok.word_index)\n",
        "# Convert new texts to sequences, observing the effect of a very limited vocabulary\n",
        "tok.texts_to_sequences(['water is hot','black coffee is cold'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-FwsL_RmVPs",
        "outputId": "8854918a-af06-435c-bea2-d036c7844c56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1], [2, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Tokenizer and set the vocabulary size to 4\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok=Tokenizer(num_words=4)\n",
        "corp=['coffee is hot','water is cold']\n",
        "tok.fit_on_texts(corp)\n",
        "print(tok.word_index)\n",
        "# Convert new texts to sequences, observing the effect of a limited vocabulary\n",
        "tok.texts_to_sequences(['water is hot','black coffee is cold'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Cn3tQ1miTs",
        "outputId": "c4042c71-9a52-4891-c14f-77ff6c1edc38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 3], [2, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6fd5950"
      },
      "source": [
        "# Project Summary\n",
        "\n",
        "This notebook demonstrates the use of the `Tokenizer` class from TensorFlow Keras for text preprocessing. The key steps covered are:\n",
        "\n",
        "1.  **Installation and Import:** Installing TensorFlow and importing the `Tokenizer`.\n",
        "2.  **Tokenizer Initialization:** Initializing the `Tokenizer` with various parameters like `num_words` and `oov_token`.\n",
        "3.  **Corpus Definition:** Defining a sample text corpus.\n",
        "4.  **Fitting the Tokenizer:** Using `fit_on_texts` to build the vocabulary and word index based on the corpus.\n",
        "5.  **Text to Sequences:** Converting text data into sequences of integer IDs using `texts_to_sequences`.\n",
        "6.  **Handling Out-of-Vocabulary (OOV) Tokens:** Demonstrating how the `oov_token` handles words not present in the vocabulary.\n",
        "7.  **Limiting Vocabulary Size:** Showing the effect of the `num_words` parameter on the vocabulary and resulting sequences.\n",
        "\n",
        "This process is a fundamental step in preparing text data for use in various natural language processing tasks, especially with neural networks."
      ]
    }
  ]
}
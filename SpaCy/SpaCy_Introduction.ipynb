{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3z4NVo5GWUZ",
        "outputId": "9edaf7b0-3e72-443b-9f64-e245d35caad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy#it can work on different type of languages"
      ],
      "metadata": {
        "id": "rQA2hsYsGZow"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBrD5ADUGqXP",
        "outputId": "070b6dc4-28f2-471a-bba9-61c78a8d15ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "X7HnhofbG3H5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt=nlp('GFG is looking for Data Science Interns')"
      ],
      "metadata": {
        "id": "TZ_32FwmQZb8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnGPYl__RNf1",
        "outputId": "bc39441e-ee0a-4e0d-8923-dcb36704b3e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GFG is looking for Data Science Interns"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Tokenization"
      ],
      "metadata": {
        "id": "xZXrSNJ-Rsc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in txt:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI7Yhk5gRTPw",
        "outputId": "d64133b3-f6f4-4d18-98ed-eabd9d7804de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GFG\n",
            "is\n",
            "looking\n",
            "for\n",
            "Data\n",
            "Science\n",
            "Interns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in txt:\n",
        "  print(type(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSBV0TtSRaV1",
        "outputId": "a6d666b9-805f-4463-87b7-7380c790e9fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.token.Token'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "<class 'spacy.tokens.token.Token'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in txt:\n",
        "  print(type(token.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Yxv3k7RgUE",
        "outputId": "be642b68-0df2-446f-86c3-f20464c0023d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=nlp(\"The cost of Iphone in U.K is 699$\")"
      ],
      "metadata": {
        "id": "U8Ipial2Rxj0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in text:\n",
        "  print(token.text)\n",
        "\n",
        "#we're writing 699$ together but the system is counting it as separate entity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGC1_7Y3SG-r",
        "outputId": "f8e71e70-33e8-4df5-9e19-98e8310d166e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "cost\n",
            "of\n",
            "Iphone\n",
            "in\n",
            "U.K\n",
            "is\n",
            "699\n",
            "$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.POS(Parts Of Speech)"
      ],
      "metadata": {
        "id": "VkvfT57HSfBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in text:\n",
        "\n",
        "  print(token.text,token.pos,token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFlMmUSfSevO",
        "outputId": "51cec7a5-197d-465e-faac-3258651df46c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 90 DET\n",
            "cost 92 NOUN\n",
            "of 85 ADP\n",
            "Iphone 96 PROPN\n",
            "in 85 ADP\n",
            "U.K 96 PROPN\n",
            "is 87 AUX\n",
            "699 93 NUM\n",
            "$ 99 SYM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference\n",
        "\n",
        "U.K is a pronoun\n",
        "\n",
        "cost is noun\n",
        "\n",
        "699 is a number\n",
        "\n",
        "$ is a symbol\n",
        "\n",
        "and the rest of the tokens are also showing some meaning"
      ],
      "metadata": {
        "id": "14X-HqscUmQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Sentence Tokenization"
      ],
      "metadata": {
        "id": "teyYJHvhTD-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent=nlp('This is sentence one. This is second sentence. This is the last one. Lets study now')"
      ],
      "metadata": {
        "id": "MnlK52pySLMq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sen in sent:\n",
        "  print(sen)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTsvV-vjTStC",
        "outputId": "b19265f8-47e1-413f-ae57-2da893f9c21b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This\n",
            "is\n",
            "sentence\n",
            "one\n",
            ".\n",
            "This\n",
            "is\n",
            "second\n",
            "sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "last\n",
            "one\n",
            ".\n",
            "Lets\n",
            "study\n",
            "now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3298daa",
        "outputId": "a6dd15fe-d57c-42ce-87be-676fb524c95f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')#knows about the language"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93a54ac2",
        "outputId": "2755e924-4483-46f3-c1eb-34d50660089c"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "\n",
        "# Use sent_tokenize to split the text into sentences\n",
        "sent_tokenize('This is sentence one. This is second sentence. This is the last one. Lets study now')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is sentence one.',\n",
              " 'This is second sentence.',\n",
              " 'This is the last one.',\n",
              " 'Lets study now']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use word_tokenize to split the text into words\n",
        "word_tokenize('This is sentence one. This is second sentence. This is the last one. Lets study now')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqKmyKa6UMPA",
        "outputId": "635fb528-f6d7-4191-b8fd-0199a206cc5f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'sentence',\n",
              " 'one',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'second',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'last',\n",
              " 'one',\n",
              " '.',\n",
              " 'Lets',\n",
              " 'study',\n",
              " 'now']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an empty cell, no code to comment on."
      ],
      "metadata": {
        "id": "V4s0SahqUOVE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f60af6",
        "outputId": "3ee9126f-03ee-430c-ffd7-52a57048ecbf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d17727e5",
        "outputId": "6a2dc9b1-0f1b-441e-c8c9-69d182c6817a"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "\n",
        "# Use sent_tokenize to split the text into sentences\n",
        "sent_tokenize('This is sentence one. This is second sentence. This is the last one. Lets study now')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is sentence one.',\n",
              " 'This is second sentence.',\n",
              " 'This is the last one.',\n",
              " 'Lets study now']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb47558e",
        "outputId": "47f0178b-a83c-4599-ae14-e2579aa9ba5a"
      },
      "source": [
        "# Use word_tokenize to split the text into words\n",
        "word_tokenize('This is sentence one. This is second sentence. This is the last one. Lets study now')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'sentence',\n",
              " 'one',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'second',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'last',\n",
              " 'one',\n",
              " '.',\n",
              " 'Lets',\n",
              " 'study',\n",
              " 'now']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cb89b19"
      },
      "source": [
        "#SUMMARY\n",
        "\n",
        "This notebook explores basic Natural Language Processing (NLP) tasks using the spaCy and NLTK libraries.\n",
        "\n",
        "**Tokenization:** We demonstrated how both libraries can be used to break down text into individual words and punctuation marks (tokens). spaCy's tokenization handled the example sentence correctly, even separating the currency symbol from the number. NLTK's tokenization was attempted but faced issues with resource downloading.\n",
        "\n",
        "**Part-of-Speech (POS) Tagging:** We showed how spaCy can identify the grammatical role of each token in a sentence (e.g., noun, verb, adjective). This helps in understanding the structure and meaning of the text.\n",
        "\n",
        "**Difference between NLTK and spaCy:**\n",
        "\n",
        "*   **NLTK (Natural Language Toolkit):** NLTK is a more traditional and comprehensive library, often used for research and teaching. It provides a wide range of algorithms and corpora, but can sometimes require more manual effort for tasks like building processing pipelines. It's known for its extensive collection of linguistic data.\n",
        "*   **spaCy:** spaCy is designed for efficiency and production use. It focuses on providing fast and accurate pre-trained models for common NLP tasks like tokenization, POS tagging, named entity recognition, and dependency parsing. spaCy has a more opinionated approach and is generally easier to use for building NLP applications.\n",
        "\n",
        "In this notebook, spaCy successfully performed tokenization and POS tagging, while NLTK encountered difficulties with resource downloads for tokenization."
      ]
    }
  ]
}